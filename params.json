{
  "name": "Distributed GPU Ray Tracing",
  "tagline": "Ye Yuan, Ken Ling",
  "body": "#  Summary\r\nWe aim to develop a fast Distributed GPU ray tracer that has the following features:  \r\n\r\n1. **Very Fast Ray Traversal**   \r\nEven for scenes with 100K+ triangles, our ray tracer can achieve a throughput of 200+ Mrays/s on GTX 680 for primary ray using a **single GPU**.\r\nAnd we can render a 1000x1000 picture of the scene with ambient occlusion at 60 FPS, while its CPU counterpart stays with 0.30 FPS.  \r\nWe made a lot of efforts to improve the efficiency of ray traversal:\r\n  * We used persistent threads to achieve better SIMD utilization.\r\n  * We used per-ray stack for ray traversal.\r\n  * We used while-while structure for node traversal and primitive intersection test.\r\n  * We optimized ray-triangle intersection and ray-box intersection.\r\n  * We also tried out speculative method, i.e., when finding a leaf, delay the primitive intersection test and keep on traversing nodes until all lanes find a leaf.  \r\nAfter all these optimizations, over ray tracer is 20x faster than original one. And we are still working on optimizing\r\nray-triangle test to decrease memory bandwidth consumed.\r\n\r\n2. **Very Fast Parallel BVH build**   \r\nWe can build BVH of scenes with 240K+ triangles in 8 ms, while the CPU build takes nearly 2 second. We use Morton codes to encode each primitive such that we can know the exact position of each node in the array. Therefore, we can build the BVH tree in fully parallel.\r\n\r\n3. **Distributed Ray Tracing across Clusters**   \r\nWe implemented a server client mechanism for the system, which allows us to render large scenes with multiple machines and GPUs. We also implemented load-balancing algorithm to cope with heterogenous nodes.\r\n\r\n4. **Versatile**  \r\nOur ray tracer support most functionalities of its CPU counterpart:  \r\n  * Monte Carlo sampling  \r\n  * Multiple types of BSDFs: refraction, glass, Fresnel reflection, e.g.\r\n  * Multiple types of Lights: point, directional, area, hemisphere, e.g.\r\n\r\n# Results\r\n\r\n## Ray Tracing\r\nThe table illustrates the GPU acceleration of Ray Tracing on a **Single GPU**.\r\nFPS represents how many times tracer can sample each pixel for an 1000x1000-pixel scene in one second.\r\nRendering time is calculated using FPS for 256 samples per pixel.\r\n\r\n![](https://github.com/Khrylx/DSGPURayTracing/blob/gh-pages/images/GPU%20Ray%20Tracing%20table.png?raw=true)\r\n\r\n## Parallel BVH Building\r\nThe table illustrates the GPU acceleration of parallel BVH tree building. As we can see from the table, the acceleration of GPU increases as the number of primitives increases.\r\n![](https://github.com/Khrylx/DSGPURayTracing/blob/gh-pages/images/GPU%20BVH%20build%20table.png?raw=true)\r\n\r\nIn this chart, Y axis denotes Nlog(N) where N is the number of primitives. X axis is the parallel BVH building time. As is shown in the chart, the parallel BVH tree building is a Nlog(N) algorithm.\r\n![](https://github.com/Khrylx/DSGPURayTracing/blob/gh-pages/images/GPU%20BVH%20build%20chart.png?raw=true)\r\n\r\n## Rendered Images\r\n![](https://github.com/Khrylx/DSGPURayTracing/blob/gh-pages/images/Screen%20Shot%20GPU%20Sat%20May%20%207%2010:56:18%202016.png?raw=true)\r\n\r\n![](https://github.com/Khrylx/DSGPURayTracing/blob/gh-pages/images/Screen%20Shot%20GPU%20Sat%20May%20%207%2011:23:08%202016.png?raw=true)\r\n\r\n# Check Point \r\nhttps://github.com/Khrylx/DSGPURayTracing/wiki",
  "note": "Don't delete this file! It's used internally to help with page regeneration."
}